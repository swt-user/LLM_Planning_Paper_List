# LLM paper list

## Planning
+ "Language Models of Code are Few-Shot Commonsense Learners", [pdf](https://arxiv.org/pdf/2210.07128.pdf)
+ "Neuro-symbolic procedural planning with commonsense prompting", [pdf](https://arxiv.org/pdf/2206.02928.pdf)
+ "Embodied Task Planning with Large Language Models", [pdf](https://arxiv.org/pdf/2307.01848.pdf)
+ "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents", [pdf](https://arxiv.org/pdf/2201.07207.pdf)
+ "Building Cooperative Embodied Agents Modularly with Large Language Models", [pdf](https://arxiv.org/pdf/2307.02485.pdf)
+ "Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners", [pdf](https://arxiv.org/pdf/2307.01928.pdf)
+ "PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning", [pdf](https://arxiv.org/pdf/2305.19472.pdf)
+ "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models", [pdf](https://voxposer.github.io/voxposer.pdf)
+ "RoCo: Dialectic Multi-Robot Collaboration with Large Language Models", [pdf](https://arxiv.org/pdf/2307.04738.pdf)
+ "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis", [pdf](https://arxiv.org/pdf/2307.12856.pdf)


## Reasoning
+ "Chain of Thought Prompting Elicits Knowledge Augmentation", [pdf](https://arxiv.org/pdf/2201.11903.pdf)
+ "UNLEASHING COGNITIVE SYNERGY IN LARGE LANGUAGE MODELS: A TASK-SOLVING AGENT THROUGH MULTI-PERSONA SELF-COLLABORATION", [pdf](https://arxiv.org/pdf/2307.05300.pdf)
+ "Self-Consistency Improves Chain of Thought Reasoning in Language Models."
+ "STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning."
+  "Large Language Models are Zero-Shot Reasoners."
+  "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models."
+  "Towards Reasoning in Large Language Models: A Survey"


## Recommendation
+ "Recommender Systems in the Era of Large Language Models (LLMs)" [pdf](https://arxiv.org/pdf/2307.02046.pdf)



## Theory of Mind
+ "Boosting Theory-of-Mind Performance in Large Language Models via Prompting"
+ "Theory of Mind May Have Spontaneously Emerged in Large Language Models"
+ "MindDial: Belief Dynamics Tracking with Theory-of-Mind Modeling for Situated Neural Dialogue Generation"
+ "Theory of Mind Reasoning in Explanation, Plan Recognition, and Assistance: Theory and Practice"
+ "What you don't know matters: An ignorance-focused investigation of theory of mind"
+ "ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind"
+ "Does ChatGPT have Theory of Mind?"
+ "Developing ChatGPT's Theory of Mind"


## Teaching small model
+ "Teaching Small Language Models to Reason", [pdf](https://arxiv.org/pdf/2212.08410.pdf)
+ "Teaching Arithmetic to Small Transformers", [pdf](https://arxiv.org/pdf/2307.03381.pdf)
+ "Distilling Reasoning Capabilities into Smaller Language Models", [pdf](https://arxiv.org/pdf/2212.00193.pdf)
+ "Explanations from Large Language Models Make Small Reasoners Better", [pdf](https://arxiv.org/pdf/2210.06726.pdf)
+ "Large Language Models Are Reasoning Teachers", [pdf](https://arxiv.org/pdf/2212.10071.pdf)
+ "Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step", [pdf](https://arxiv.org/pdf/2306.14050.pdf)
+ "PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning", [pdf](https://arxiv.org/pdf/2305.19472.pdf)
+ "Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks", [pdf](https://arxiv.org/pdf/2305.18395.pdf)
+ "STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning", [pdf]()
+ "Generating Efficient Training Data via LLM-based Attribute Manipulation", [pdf]()


## Simulation
+ "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies"
